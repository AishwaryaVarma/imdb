{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2020-02-22 06:43:54--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  5.61MB/s    in 19s     \n",
      "\n",
      "2020-02-22 06:44:15 (4.13 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'may contain spoilers!!!! so i watched this movie last night on LMN (Lifetime Movie Network) which is NOT known for showing quality movies. THIS MOVIE IS AWFUL! i am still amazed that i watched the entire thing, as it was terrible. could this movie contain any more stereotypes? (harping jewish mother who wants son to be a doctor, catholic family with priest sons, big big crucifixes in every room shown in the catholic family\\'s house, mexican whores, \"bad\" guy who is really a softie at heart, incredibly bad country accents) GAG!!!! i was at first intrigued by the fact that i had never heard of this movie and after seeing that cheryl pollack and corin nemec were in it, i decided to stay awake until 4am to watch it. anyway, the only redeeming thing about this movie is madchen amick\\'s beauty. i suppose pollack\\'s and nemec\\'s acting is okay, but they have a horrid script to work with. unlike the other reviewer who commented on the lack of texan accents (the movie is supposed to take place in austin and very few people there have a twang) i think that the accents were there (in supporting characters like mary margaret\\'s date and john) and were unnecessary. they were also very very bad. i am so very tired of hollywood \"southern\" accents that sound nothing like the area where the accent is supposed to be from. and since it was supposed to take place in austin and shooting movies there in 1991 would not have been expensive, i fully expected there to be familiar shots of the town: the beautiful capitol building, the UT tower lit up for a winning football game, etc. none of these things were there. also, it takes about 5-6 hours to drive to mexico from austin. at one point in the movie, michael and his posse take off for mexico to lose their virginities and are able to drive off when it is dark (during the summer and early fall it doesn\\'t get dark in austin until 9pm or so), spend time in mexico getting drunk and having sex with mexican (is there any other kind?) whores, and then return to austin by dawn. while this is theoretically possible it is NOT very likely. and if anyone has started school in the hill country (usually the third week of august, but may have been in september in 1960) they know that unless they want to pass out from heat stroke they DO NOT wear their letter jackets!!!!! in august and september in austin and the surrounding areas it is 90+ degrees. only people with no body temperature would be stupid enough to wear sweaters or letter jackets on the first day of school. all in all, a very bad made for tv movie experience.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may',\n",
       " 'contain',\n",
       " 'spoiler',\n",
       " 'watch',\n",
       " 'movi',\n",
       " 'last',\n",
       " 'night',\n",
       " 'lmn',\n",
       " 'lifetim',\n",
       " 'movi',\n",
       " 'network',\n",
       " 'known',\n",
       " 'show',\n",
       " 'qualiti',\n",
       " 'movi',\n",
       " 'movi',\n",
       " 'aw',\n",
       " 'still',\n",
       " 'amaz',\n",
       " 'watch',\n",
       " 'entir',\n",
       " 'thing',\n",
       " 'terribl',\n",
       " 'could',\n",
       " 'movi',\n",
       " 'contain',\n",
       " 'stereotyp',\n",
       " 'harp',\n",
       " 'jewish',\n",
       " 'mother',\n",
       " 'want',\n",
       " 'son',\n",
       " 'doctor',\n",
       " 'cathol',\n",
       " 'famili',\n",
       " 'priest',\n",
       " 'son',\n",
       " 'big',\n",
       " 'big',\n",
       " 'crucifix',\n",
       " 'everi',\n",
       " 'room',\n",
       " 'shown',\n",
       " 'cathol',\n",
       " 'famili',\n",
       " 'hous',\n",
       " 'mexican',\n",
       " 'whore',\n",
       " 'bad',\n",
       " 'guy',\n",
       " 'realli',\n",
       " 'softi',\n",
       " 'heart',\n",
       " 'incred',\n",
       " 'bad',\n",
       " 'countri',\n",
       " 'accent',\n",
       " 'gag',\n",
       " 'first',\n",
       " 'intrigu',\n",
       " 'fact',\n",
       " 'never',\n",
       " 'heard',\n",
       " 'movi',\n",
       " 'see',\n",
       " 'cheryl',\n",
       " 'pollack',\n",
       " 'corin',\n",
       " 'nemec',\n",
       " 'decid',\n",
       " 'stay',\n",
       " 'awak',\n",
       " '4am',\n",
       " 'watch',\n",
       " 'anyway',\n",
       " 'redeem',\n",
       " 'thing',\n",
       " 'movi',\n",
       " 'madchen',\n",
       " 'amick',\n",
       " 'beauti',\n",
       " 'suppos',\n",
       " 'pollack',\n",
       " 'nemec',\n",
       " 'act',\n",
       " 'okay',\n",
       " 'horrid',\n",
       " 'script',\n",
       " 'work',\n",
       " 'unlik',\n",
       " 'review',\n",
       " 'comment',\n",
       " 'lack',\n",
       " 'texan',\n",
       " 'accent',\n",
       " 'movi',\n",
       " 'suppos',\n",
       " 'take',\n",
       " 'place',\n",
       " 'austin',\n",
       " 'peopl',\n",
       " 'twang',\n",
       " 'think',\n",
       " 'accent',\n",
       " 'support',\n",
       " 'charact',\n",
       " 'like',\n",
       " 'mari',\n",
       " 'margaret',\n",
       " 'date',\n",
       " 'john',\n",
       " 'unnecessari',\n",
       " 'also',\n",
       " 'bad',\n",
       " 'tire',\n",
       " 'hollywood',\n",
       " 'southern',\n",
       " 'accent',\n",
       " 'sound',\n",
       " 'noth',\n",
       " 'like',\n",
       " 'area',\n",
       " 'accent',\n",
       " 'suppos',\n",
       " 'sinc',\n",
       " 'suppos',\n",
       " 'take',\n",
       " 'place',\n",
       " 'austin',\n",
       " 'shoot',\n",
       " 'movi',\n",
       " '1991',\n",
       " 'would',\n",
       " 'expens',\n",
       " 'fulli',\n",
       " 'expect',\n",
       " 'familiar',\n",
       " 'shot',\n",
       " 'town',\n",
       " 'beauti',\n",
       " 'capitol',\n",
       " 'build',\n",
       " 'ut',\n",
       " 'tower',\n",
       " 'lit',\n",
       " 'win',\n",
       " 'footbal',\n",
       " 'game',\n",
       " 'etc',\n",
       " 'none',\n",
       " 'thing',\n",
       " 'also',\n",
       " 'take',\n",
       " '5',\n",
       " '6',\n",
       " 'hour',\n",
       " 'drive',\n",
       " 'mexico',\n",
       " 'austin',\n",
       " 'one',\n",
       " 'point',\n",
       " 'movi',\n",
       " 'michael',\n",
       " 'poss',\n",
       " 'take',\n",
       " 'mexico',\n",
       " 'lose',\n",
       " 'virgin',\n",
       " 'abl',\n",
       " 'drive',\n",
       " 'dark',\n",
       " 'summer',\n",
       " 'earli',\n",
       " 'fall',\n",
       " 'get',\n",
       " 'dark',\n",
       " 'austin',\n",
       " '9pm',\n",
       " 'spend',\n",
       " 'time',\n",
       " 'mexico',\n",
       " 'get',\n",
       " 'drunk',\n",
       " 'sex',\n",
       " 'mexican',\n",
       " 'kind',\n",
       " 'whore',\n",
       " 'return',\n",
       " 'austin',\n",
       " 'dawn',\n",
       " 'theoret',\n",
       " 'possibl',\n",
       " 'like',\n",
       " 'anyon',\n",
       " 'start',\n",
       " 'school',\n",
       " 'hill',\n",
       " 'countri',\n",
       " 'usual',\n",
       " 'third',\n",
       " 'week',\n",
       " 'august',\n",
       " 'may',\n",
       " 'septemb',\n",
       " '1960',\n",
       " 'know',\n",
       " 'unless',\n",
       " 'want',\n",
       " 'pass',\n",
       " 'heat',\n",
       " 'stroke',\n",
       " 'wear',\n",
       " 'letter',\n",
       " 'jacket',\n",
       " 'august',\n",
       " 'septemb',\n",
       " 'austin',\n",
       " 'surround',\n",
       " 'area',\n",
       " '90',\n",
       " 'degre',\n",
       " 'peopl',\n",
       " 'bodi',\n",
       " 'temperatur',\n",
       " 'would',\n",
       " 'stupid',\n",
       " 'enough',\n",
       " 'wear',\n",
       " 'sweater',\n",
       " 'letter',\n",
       " 'jacket',\n",
       " 'first',\n",
       " 'day',\n",
       " 'school',\n",
       " 'bad',\n",
       " 'made',\n",
       " 'tv',\n",
       " 'movi',\n",
       " 'experi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "           vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "         features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
